{
  "hash": "6614abf684669216b49160be1770e63d",
  "result": {
    "markdown": "---\ntitle: Probability Theory - Sentiment Analysis with Naive Bayes\n---\n\nPerforming sentiment analysis on Twitter tweets using the Naive Bayes algorithm.\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import classification_report\n```\n:::\n\n\n## Loading and Preprocessing the Data\n\nFirst, we load the data and explore its structure.\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\n# Load the dataset\ndata = pd.read_csv('./twitter_training.csv' )\n\n# Display the first few rows of the dataset\ndata.head()\n```\n\n::: {.cell-output .cell-output-display execution_count=2}\n```{=html}\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Topic</th>\n      <th>Sentiment</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will murder yo...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>I am coming to the borders and I will kill you...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands and i will kill you ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im coming on borderlands and i will murder you...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Borderlands</td>\n      <td>Positive</td>\n      <td>im getting on borderlands 2 and i will murder ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n```\n:::\n:::\n\n\nNext, we preprocess the data by removing rows with missing text values.\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\n# Remove rows with missing values in the 'text' column\ncleaned_data = data.dropna(subset=['Text'])\n```\n:::\n\n\n## Preparing the Data for Modeling\n\nWe split the data into training and testing sets.\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\n# Splitting the dataset into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(cleaned_data['Text'], cleaned_data['Sentiment'], test_size=0.2, random_state=42)\n```\n:::\n\n\n## Building and Training the Naive Bayes Model\n\nWe use a pipeline to vectorize the text and train the model.\n\n::: {.cell tags='[]' execution_count=5}\n``` {.python .cell-code}\n# Preprocessing and vectorization using a pipeline\nmodel = make_pipeline(\n    TfidfVectorizer(),\n    MultinomialNB()\n)\n\n# Training the model\nmodel.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=5}\n```\nPipeline(steps=[('tfidfvectorizer', TfidfVectorizer()),\n                ('multinomialnb', MultinomialNB())])\n```\n:::\n:::\n\n\n## Evaluating the Model\n\nFinally, we evaluate the model's performance on the test set.\n\n::: {.cell tags='[]' execution_count=6}\n``` {.python .cell-code}\n# Evaluating the model\npredictions = model.predict(X_test)\nreport = classification_report(y_test, predictions)\nprint(report)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n              precision    recall  f1-score   support\n\n  Irrelevant       0.95      0.38      0.55      2696\n    Negative       0.63      0.91      0.75      4380\n     Neutral       0.85      0.60      0.70      3605\n    Positive       0.69      0.82      0.75      4119\n\n    accuracy                           0.71     14800\n   macro avg       0.78      0.68      0.69     14800\nweighted avg       0.76      0.71      0.70     14800\n\n```\n:::\n:::\n\n\n## Testing the Model with New Data and Visualization\n\nNow, we will test the model with new data and visualize the accuracy of the model.\n\n::: {.cell tags='[]' execution_count=7}\n``` {.python .cell-code}\n# Example new data to test\nnew_data = [\"This movie was fantastic, I loved it!\",\n            \"The movie was okay, but I think the ending could be better\",\n            \"I did not like the movie, it was boring and too long\"]\n\n# Predicting the sentiment of the new data\nnew_predictions = model.predict(X_test)\nprint(X_test)\nnew_predictions\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n61413    Looks to me like he failed to check out the wa...\n44887    Wow, it takes all sorts of crazy people out th...\n73662    Nvidia Unveils The Worldâ€™s Fastest Gaming Moni...\n36694    Huge radio play here. Reinvention / Corporate ...\n2308                            SO I HAPPY WHO ABOUT THIS.\n                               ...                        \n12630    @Ronnie2K where is all my Mamba Edition extras...\n49615               Sell 700k fifa coins fucking this game\n12322    @NBA2K $ 107 for a four game break and I can't...\n4355     has called me a madman.. I understood right fr...\n52612       RDR2 at 31m makes really this boy super happy.\nName: Text, Length: 14800, dtype: object\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\narray(['Negative', 'Positive', 'Neutral', ..., 'Negative', 'Irrelevant',\n       'Positive'], dtype='<U10')\n```\n:::\n:::\n\n\n## Data Visualization\n\nCreating a confusion matrix to illustrate the predictions made by the classifier on the test sets.\n\n::: {.cell tags='[]' execution_count=8}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nimport seaborn as sns\n\nnew_data_actual_labels = y_test\n\n# Calculating accuracy\naccuracy = accuracy_score(new_data_actual_labels, new_predictions)\n\n# Creating a confusion matrix\nconf_matrix = confusion_matrix(new_data_actual_labels, new_predictions)\n\n# Plotting the confusion matrix\nsns.heatmap(conf_matrix, annot=True, fmt='g', cmap='Blues')\nplt.title(f'Model Accuracy: {accuracy:.2f}')\nplt.xlabel('Predicted labels')\nplt.ylabel('True labels')\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](probability_theory-checkpoint_files/figure-html/cell-9-output-1.png){width=564 height=449}\n:::\n:::\n\n\n## Interpretation\n\nThe four labels represent four sentiments: Positive, Negative, Neutral, and Irrelevant The diagonal entries in the matrix tell us how many predictions for each class were correct: the model correctly predicted class 0 for 1037 instances, class 1 for 3985 instances, class 2 for 2174 instances, and class 3 for 3370 instances. All of the off-diagonal entries represent misclassifications.\n\nOverall, the model is most accurate with classes 1 and 3, where the diagonal values are high relative to the off-diagonal values. The model seems to struggle the most with accurately classifying class 2.\n\n",
    "supporting": [
      "probability_theory-checkpoint_files"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}