{
  "hash": "a354a8f186ed00317e1b8e3d5b0bf054",
  "result": {
    "markdown": "---\ntitle: Linear Regression on Height and Weight data for 18 year olds\n---\n\n-   Dataset - Height and Weight data for 18 year olds\n-   Plot original data on scatterplot\n-   Split data into training and test sets\n-   Use regression to see if the data is linear\n-   Get coefficients of linear equation and r-value\n    -   low r-value -- linear regression is not the best fit for the data\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport sys\n\nassert sys.version_info >= (3, 7)\n```\n:::\n\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n```\n:::\n\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\nimport pandas as pd\n\ndf = pd.read_csv(\"./SOCR-HeightWeight-checkpoint.csv\")\n\n#df['Weight(Pounds)']\ndf['Height(Inches)']\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n0        65.78331\n1        71.51521\n2        69.39874\n3        68.21660\n4        67.78781\n           ...   \n24995    69.50215\n24996    64.54826\n24997    64.69855\n24998    67.52918\n24999    68.87761\nName: Height(Inches), Length: 25000, dtype: float64\n```\n:::\n:::\n\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\nimport numpy as np\n```\n:::\n\n\n## Plot original data on scatterplot\n\n::: {.cell tags='[]' execution_count=5}\n``` {.python .cell-code}\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nplt.figure(figsize=(10, 4))\n\nX = df['Weight(Pounds)'][0:200]\ny = df['Height(Inches)'][0:200]\nplt.scatter(X, y, marker='*')\nplt.xlabel(\"$weight$\")\nplt.ylabel(\"$height$\", rotation=90)\nplt.axis([90, 160, 60, 75])\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-checkpoint_files/figure-html/cell-6-output-1.png){width=828 height=363}\n:::\n:::\n\n\n## Split data into testing and training sets\n\n::: {.cell tags='[]' execution_count=6}\n``` {.python .cell-code}\nfrom sklearn import datasets, linear_model\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n# Split the data into training/testing sets\nX_train = X[:-100]\nX_test = X[100:]\n\n# Split the targets into training/testing sets\ny_train = y[:-100]\ny_test = y[100:]\n```\n:::\n\n\n## Fit the data to a linear regression\n\n::: {.cell tags='[]' execution_count=7}\n``` {.python .cell-code}\n# Create linear regression object\nregr = linear_model.LinearRegression()\n\n# Train the model using the training sets\nX_train=np.array(X_train).reshape(-1, 1) # reshaping to 2D array\n#y_train=np.array(y_train).reshape(-1, 1) # reshaping to 2D array\nregr.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=7}\n```\nLinearRegression()\n```\n:::\n:::\n\n\n::: {.cell tags='[]' execution_count=8}\n``` {.python .cell-code}\n# Make predictions using the testing set\nX_test=np.array(X_test).reshape(-1, 1) # reshaping to 2D array\ny_pred = regr.predict(X_test)\n```\n:::\n\n\n## Get coefficients of linear-equation\n\n::: {.cell tags='[]' execution_count=9}\n``` {.python .cell-code}\n# The coefficients\nprint(\"Coefficients: \\n\", regr.coef_)\nprint(\"y-intercept: \\n\", regr.intercept_)\n# The mean squared error\nprint(\"Mean squared error: %.2f\" % mean_squared_error(y_test, y_pred))\n# The coefficient of determination: 1 is perfect prediction\nprint(\"Coefficient of determination: %.2f\" % r2_score(y_test, y_pred))\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nCoefficients: \n [0.07991179]\ny-intercept: \n 57.8157253513461\nMean squared error: 2.83\nCoefficient of determination: 0.31\n```\n:::\n:::\n\n\nThe low r-value tells us that a linear regression is not the best fit for this data. We know that this model will not make very accurate predictions.\n\n::: {.cell tags='[]' execution_count=10}\n``` {.python .cell-code}\n# Plot outputs\nimport matplotlib.pyplot as plt\n\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\nplt.figure(figsize=(10, 4))\n\nplt.xlabel(\"$weight$\")\nplt.ylabel(\"$height$\", rotation=90)\nplt.axis([90, 160, 60, 75])\n\nplt.scatter(X_test, y_test, marker='*')\nplt.plot(X_test, y_pred, color=\"black\", linewidth=2)\n\nplt.grid()\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](regression-checkpoint_files/figure-html/cell-11-output-1.png){width=828 height=363}\n:::\n:::\n\n\n## Trying other regressions to find a better fit\n\n::: {.cell tags='[]' execution_count=11}\n``` {.python .cell-code}\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso\nfrom sklearn.metrics import mean_squared_error, r2_score\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.pipeline import make_pipeline\n\n# Fit a polynomial regression model\ndegree = 2  # You can try different degrees\npoly_reg = make_pipeline(PolynomialFeatures(degree), LinearRegression())\npoly_reg.fit(X_train, y_train)\n\n# Fit a Ridge regression model\nridge_reg = Ridge(alpha=1.0)  # Alpha is a hyperparameter that you can tune\nridge_reg.fit(X_train, y_train)\n\n# Fit a Lasso regression model\nlasso_reg = Lasso(alpha=0.1)  # Alpha is a hyperparameter that you can tune\nlasso_reg.fit(X_train, y_train)\n\n# Evaluate the models using test data\nmodels = {'Polynomial': poly_reg, 'Ridge': ridge_reg, 'Lasso': lasso_reg}\nfor name, model in models.items():\n    y_pred = model.predict(X_test)\n    mse = mean_squared_error(y_test, y_pred)\n    r2 = r2_score(y_test, y_pred)\n    print(f'{name} Regression - MSE: {mse:.2f}, R²: {r2:.2f}')\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nPolynomial Regression - MSE: 2.82, R²: 0.31\nRidge Regression - MSE: 2.83, R²: 0.31\nLasso Regression - MSE: 2.83, R²: 0.31\n```\n:::\n:::\n\n\nAll three models show very similar performance in terms of both MSE and R². The Ridge and Lasso's similar performance to polynomial regression could imply that higher-order terms do not significantly improve the model, or the chosen degree for the polynomial model was not high enough to capture more complex relationships.\n\n",
    "supporting": [
      "regression-checkpoint_files"
    ],
    "filters": [],
    "includes": {}
  }
}