{
  "hash": "70bdc2ada07fffff49f1205d332206a1",
  "result": {
    "markdown": "---\ntitle: Classification - Classifying Texts in Different Languages\n---\n\n::: {.cell tags='[]' execution_count=1}\n``` {.python .cell-code}\nimport sys\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nassert sys.version_info >= (3, 7)\n```\n:::\n\n\n::: {.cell tags='[]' execution_count=2}\n``` {.python .cell-code}\nfrom packaging import version\nimport sklearn\n\nassert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\")\n```\n:::\n\n\nFirst we read the input data. The data contains two columns: Text and Language\n\n::: {.cell tags='[]' execution_count=3}\n``` {.python .cell-code}\nfrom sklearn.datasets import fetch_openml\n\ndf = pd.read_csv(\"./Language Detection-checkpoint.csv\")\n```\n:::\n\n\n::: {.cell tags='[]' execution_count=4}\n``` {.python .cell-code}\nprint(df.keys())\ndf['Text']\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nIndex(['Text', 'Language'], dtype='object')\n```\n:::\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\n0         Nature, in the broadest sense, is the natural...\n1        \"Nature\" can refer to the phenomena of the phy...\n2        The study of nature is a large, if not the onl...\n3        Although humans are part of nature, human acti...\n4        [1] The word nature is borrowed from the Old F...\n                               ...                        \n10332    ನಿಮ್ಮ ತಪ್ಪು ಏನು ಬಂದಿದೆಯೆಂದರೆ ಆ ದಿನದಿಂದ ನಿಮಗೆ ಒ...\n10333    ನಾರ್ಸಿಸಾ ತಾನು ಮೊದಲಿಗೆ ಹೆಣಗಾಡುತ್ತಿದ್ದ ಮಾರ್ಗಗಳನ್...\n10334    ಹೇಗೆ ' ನಾರ್ಸಿಸಿಸಮ್ ಈಗ ಮರಿಯನ್ ಅವರಿಗೆ ಸಂಭವಿಸಿದ ಎ...\n10335    ಅವಳು ಈಗ ಹೆಚ್ಚು ಚಿನ್ನದ ಬ್ರೆಡ್ ಬಯಸುವುದಿಲ್ಲ ಎಂದು ...\n10336    ಟೆರ್ರಿ ನೀವು ನಿಜವಾಗಿಯೂ ಆ ದೇವದೂತನಂತೆ ಸ್ವಲ್ಪ ಕಾಣು...\nName: Text, Length: 10337, dtype: object\n```\n:::\n:::\n\n\n## Plot languages against the number of texts in that language\n\n::: {.cell execution_count=5}\n``` {.python .cell-code}\nplt.rc('font', size=14)\nplt.rc('axes', labelsize=14, titlesize=14)\nplt.rc('legend', fontsize=14)\nplt.rc('xtick', labelsize=10)\nplt.rc('ytick', labelsize=10)\n\nplt.bar(x=df['Language'].value_counts().index, height=df['Language'].value_counts())\nplt.xlabel('Language')\nplt.ylabel('Number of Texts')\nplt.xticks(rotation=90)\nplt.show()\n```\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-6-output-1.png){width=607 height=497}\n:::\n:::\n\n\n## Preprocess the Data\n\nMake sure the texts only contain alphabetical characters\n\n::: {.cell execution_count=6}\n``` {.python .cell-code}\n# Text data preprocessing\n# Uniform case, remove symbols and whitespace\ndf['Text'] = df['Text'].str.lower()\ndf['Text'] = df['Text'].str.replace(r'[\\([{})\\]!@#$,\"%^*?:;~`0-9]', '', regex=True)\ndf['Text'] = df['Text'].str.strip()\n```\n:::\n\n\n## Using TF-IDF: Term Frequency Inverse Document Frequency\n\nWe use TF-IDF to transform text into numeric data. This measures the originality of a word by comparing the number of times a particular word appears in a text versus the number of texts that word appears in\n\n::: {.cell tags='[]' execution_count=7}\n``` {.python .cell-code}\n# The TF-IDF vectorizer initializing \nfrom sklearn.feature_extraction.text import TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_vectorizer.fit(df['Text'])\nX_tfidf = tfidf_vectorizer.transform(df['Text'])\n```\n:::\n\n\nWe use a Label Encoder to convert each language name to a number.\n\n::: {.cell tags='[]' execution_count=8}\n``` {.python .cell-code}\n# Encoding the target labels (languages)\nfrom sklearn.preprocessing import LabelEncoder\nlabel_encoder = LabelEncoder()\ny_encoded = label_encoder.fit_transform(df['Language'])\n```\n:::\n\n\n## Split the data into testing and training sets\n\n::: {.cell tags='[]' execution_count=9}\n``` {.python .cell-code}\nX_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:], y_encoded[:5000], y_encoded[5000:]\n#X_train, X_test, y_train, y_test = X_tfidf[:5000], X_tfidf[5000:10000], y_num[:5000], y_num[5000:10000]\nX_train_english = (X_train == 1) # True for all English texts, English = 1\n\nprint(y_encoded)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n[3 3 3 ... 9 9 9]\n```\n:::\n:::\n\n\n## Use SGD (Stochastic Gradient Descent) to classify and make predictions\n\n::: {.cell execution_count=10}\n``` {.python .cell-code}\n# Training binary classifier\nfrom sklearn.linear_model import SGDClassifier\nimport numpy as np\n\nsgd_clf = SGDClassifier(random_state=42)\nsgd_clf.fit(X_train, y_train)\n```\n\n::: {.cell-output .cell-output-display execution_count=10}\n```\nSGDClassifier(random_state=42)\n```\n:::\n:::\n\n\n## Prediction Function\n\nThis prediction function takes a text as input, and outputs the predicted language that the text is written in.\n\n::: {.cell tags='[]' execution_count=11}\n``` {.python .cell-code}\n# predict languge given text \ndef predict_language(input_text):\n    input_text = input_text.lower() # preprocess input text\n    input_tfidf = tfidf_vectorizer.transform([input_text]) # convert input to numeric data\n    predicted = sgd_clf.predict(input_tfidf) # get encoded prediction\n    predicted_language = label_encoder.inverse_transform(predicted) # decode prediction\n    return predicted_language\n\n#predict_language(X[1400])\n```\n:::\n\n\n## Data Visualization\n\nCreating a confusion matrix to illustrate the predictions made by the classifier.\n\n::: {.cell tags='[]' execution_count=12}\n``` {.python .cell-code}\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import LabelEncoder\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Fit the label encoder on all possible labels\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(df['Language'])  # Fit on the entire label set\n\n# Now transform the test labels\nLanguage_test_encoded = label_encoder.transform(df['Language'][5000:])\n\nText_test = df['Text'][5000:]\npredicted_languages = [predict_language(text) for text in Text_test]\n\n# Transform the predicted languages back to the encoded form\npredicted_languages_encoded = label_encoder.transform(predicted_languages)\n\n# Comparing the predicted languages with the actual labels\nresults_df = pd.DataFrame({'Actual': Language_test_encoded, 'Predicted': predicted_languages_encoded})\n\n# Construct the confusion matrix manually\nconf_matrix = confusion_matrix(results_df['Actual'], results_df['Predicted'])\n# Get all unique labels present in the actual and predicted labels\nall_labels = np.union1d(results_df['Actual'], results_df['Predicted'])\n\n# Convert to DataFrame for easier plotting\n# Use all_labels for both the index and columns to account for every possible class\nconf_matrix_df = pd.DataFrame(conf_matrix, \n                              index=all_labels, \n                              columns=all_labels)\n\n# Plot the confusion matrix\nplt.figure(figsize=(10, 8))\nsns.heatmap(conf_matrix_df, annot=True, fmt='d', square=True)\nplt.title('Confusion Matrix of Language Predictions')\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.show()\n\n# Calculate accuracy\naccuracy = (results_df['Actual'] == results_df['Predicted']).mean()\nprint(f'Accuracy of the predict_language function: {accuracy:.2f}')\n```\n\n::: {.cell-output .cell-output-stderr}\n```\nC:\\Users\\veda\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:133: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n  y = column_or_1d(y, warn=True)\n```\n:::\n\n::: {.cell-output .cell-output-display}\n![](classification_files/figure-html/cell-13-output-2.png){width=758 height=679}\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nAccuracy of the predict_language function: 0.07\n```\n:::\n:::\n\n\n## Interpretation\n\nFrom the confusion matrix, we can see that the model is very good at predicting some languages (like the one labeled '8', with 341 correct predictions). Some languages are often confused with others, such as labels '5', '6', '9', and '12', indicating potential areas for model improvement. The accuracy of this model could potentially be improved by using more training data and by trying different algorithms.\n\n",
    "supporting": [
      "classification_files"
    ],
    "filters": [],
    "includes": {}
  }
}